{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5F5xBxWEY4BaOgqfZG+cX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annabellesauve/Empathy-Detection/blob/main/Pre_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iZ7h1FcSfXA"
      },
      "source": [
        "\n",
        "**bold text**\n",
        "# Configurations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMAnFq7Q4pat"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ekpHU_S7iTl",
        "outputId": "6f1cf222-dd23-4c28-dbb3-cb38de3f26f3"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgUTqJXhBEO7"
      },
      "source": [
        "#Set the path to the data folder, datafile and output folder and files\n",
        "root_folder = '/content/drive/My Drive'\n",
        "data_folder = os.path.abspath(os.path.join(root_folder, 'Empathy Detection Project/empathy_dataset'))\n",
        "model_folder = os.path.abspath(os.path.join(root_folder, 'Empathy Detection Project/pre-training model'))\n",
        "output_folder = os.path.abspath(os.path.join(root_folder, 'Empathy Detection Project/pre-tarining output'))\n",
        "tokenizer_folder = os.path.abspath(os.path.join(root_folder, 'Empathy Detection Project/tokenizer'))\n",
        "\n",
        "test_filename='interpretations_Test-reddit.csv'\n",
        "datafile= 'interpretations-reddit.csv'\n",
        "outputfile = 'interpretationsafterMLM.csv'\n",
        "\n",
        "datafile_path = os.path.abspath(os.path.join(data_folder,datafile))\n",
        "testfile_path = os.path.abspath(os.path.join(data_folder,test_filename))\n",
        "outputfile_path = os.path.abspath(os.path.join(output_folder,outputfile))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XTJ87ycUDuy"
      },
      "source": [
        "# Load Training and Testing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtfdxU5zH983",
        "outputId": "0e6903a0-35d3-4049-a52c-e2497c4a20b2"
      },
      "source": [
        "# Load the train dataset\n",
        "train_df=pd.read_csv(datafile_path, header=0, usecols=[2,3])\n",
        "# Show the count of rows\n",
        "print('Num Examples: ',len(train_df))\n",
        "print('Null Values\\n', train_df.isna().sum())\n",
        "print(train_df)\n",
        "# Drop rows with Null values \n",
        "train_df.dropna(inplace=True)\n",
        "print('Num Examples: ',len(train_df))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Examples:  3084\n",
            "Null Values\n",
            " seeker_post      0\n",
            "response_post    0\n",
            "dtype: int64\n",
            "                                            seeker_post                                      response_post\n",
            "0         Help. Help me. I dunno what I'm doing anymore  That's pretty vague, do you not know what you'...\n",
            "1     I'm done saying I love you to her because I do...  idk what a Red pill means exactly but my advic...\n",
            "2     Always feel like I'm being criticized and mock...  I think it's social anxiety , that creates par...\n",
            "3     My diet becomes fucked when i get depressed.. ...  By any chance do you think you're in a loop. J...\n",
            "4     I hate not knowing why. I was diagnosed with d...  depression. not sadness which is caused by som...\n",
            "...                                                 ...                                                ...\n",
            "3079  does anyone else keep forgetting stuff the nee...  All day, every day. It's definitely not just y...\n",
            "3080  What does depression feel like?. Honest questi...  like being stuck in a black hole. At times you...\n",
            "3081  I'm to scared to commit suicide.. All I can fe...  I probably would have considered bringing harm...\n",
            "3082  I just want to disappear but I don't want to h...                        People barely notice me too\n",
            "3083  26 year old male, living at home, low income j...  I see what you mean, but imagine being 26, liv...\n",
            "\n",
            "[3084 rows x 2 columns]\n",
            "Num Examples:  3084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJb9myAER2F0",
        "outputId": "7befb3cc-a560-4bec-9792-2fdeecd671b4"
      },
      "source": [
        "# Load the test dataset \n",
        "test_df=pd.read_csv(testfile_path, header=0)\n",
        "print('Num Examples: ',len(test_df))\n",
        "print('Null Values\\n', test_df.isna().sum())\n",
        "print(test_df)\n",
        "# there are no null values"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Examples:  1001\n",
            "Null Values\n",
            " seeker_post      0\n",
            "response_post    0\n",
            "dtype: int64\n",
            "                                            seeker_post                                      response_post\n",
            "0         Help. Help me. I dunno what I'm doing anymore  That's pretty vague, do you not know what you'...\n",
            "1     I'm done saying I love you to her because I do...  idk what a Red pill means exactly but my advic...\n",
            "2     Always feel like I'm being criticized and mock...  I think it's social anxiety , that creates par...\n",
            "3     My diet becomes fucked when i get depressed.. ...  By any chance do you think you're in a loop. J...\n",
            "4     I hate not knowing why. I was diagnosed with d...  depression. not sadness which is caused by som...\n",
            "...                                                 ...                                                ...\n",
            "996   There is literally nothing anyone can say to m...  I'm not going to lie to you and say there's a ...\n",
            "997   Circle of my life. I'm depressed and fat, so I...  Same thing happened to me. I used to work for ...\n",
            "998   Going to get high tomorrow. Welp, tired of liv...         Well I sincerely hope you have a good time\n",
            "999   My birthday is today. I want to be pitiful and...  Happy Birthday! Happy Birthday! Happy Birthday...\n",
            "1000  meds. should i take my meds? i mean the anti d...  Yes. Because when you take them and then sudde...\n",
            "\n",
            "[1001 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMKwdXOyUQKJ"
      },
      "source": [
        "# Create the dataset to train a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7VCYgRPSaWN"
      },
      "source": [
        "\n",
        "\n",
        "# Drop the files from the output dir\n",
        "txt_files_dir = \"./text_split\"\n",
        "!rm -rf {txt_files_dir}\n",
        "!mkdir {txt_files_dir}\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QZX4wXcVbWp"
      },
      "source": [
        "# Store values in a dataframe column (Series object) to files, one file per record\n",
        "def column_to_files(column, prefix, txt_files_dir):\n",
        "    # The prefix is a unique ID to avoid to overwrite a text file\n",
        "    i=prefix\n",
        "    #For every value in the df, with just one column\n",
        "    for row in column.to_list():\n",
        "      # Create the filename using the prefix ID\n",
        "      file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n",
        "      try:\n",
        "        # Create the file and write the column text to it\n",
        "        f = open(file_name, 'wb')\n",
        "        f.write(row.encode('utf-8'))\n",
        "        f.close()\n",
        "      except Exception as e:  #catch exceptions(for eg. empty rows)\n",
        "        print(row, e) \n",
        "      i+=1\n",
        "    # Return the last ID\n",
        "    return i"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnSOL5uOVtO0",
        "outputId": "29c86144-67a2-4f8c-81b9-e48327b0a62a"
      },
      "source": [
        "data = train_df[\"response_post\"]\n",
        "# Removing the end of line character \\n\n",
        "data = data.replace(\"\\n\",\" \")\n",
        "# Set the ID to 0\n",
        "prefix=0\n",
        "# Create a file for every description value\n",
        "prefix = column_to_files(data, prefix, txt_files_dir)\n",
        "# Print the last ID\n",
        "print(prefix)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evk2ZquEZYXK",
        "outputId": "f3587dfa-8d54-4666-f407-807365efa1c7"
      },
      "source": [
        "data = test_df[\"response_post\"]\n",
        "# Removing the end of line character \\n\n",
        "data = data.replace(\"\\n\",\" \")\n",
        "print(len(data))\n",
        "# Create a file for every description value\n",
        "prefix = column_to_files(data, prefix, txt_files_dir)\n",
        "print(prefix)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001\n",
            "4085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfJZjRPHwP6_",
        "outputId": "85b41767-0db6-4944-c342-fe7decea6303"
      },
      "source": [
        "data = train_df[\"seeker_post\"]\n",
        "data = data.replace(\"\\n\",\" \")\n",
        "print(len(data))\n",
        "prefix = column_to_files(data, prefix, txt_files_dir)\n",
        "print(prefix)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3084\n",
            "7169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBuGo7Ddkc3-"
      },
      "source": [
        "# Train Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkd9Wf83X5H6",
        "outputId": "53425514-ca72-4255-ed79-dad113164a1f"
      },
      "source": [
        "# Install `transformers` from master\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n",
        "# transformers version at notebook update --- 2.11.0\n",
        "# tokenizers version at notebook update --- 0.8.0rc1\n",
        "import tensorflow as tf \n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-xt48pdr8\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-xt48pdr8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 402 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 12.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 31.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.1.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3254938 sha256=b45e899a58656d1cbee5f7dac71a5c6a881ee6232a519bbcbe92cbcf943504a4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-aue7w2qr/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0.dev0\n",
            "tokenizers                    0.10.3\n",
            "transformers                  4.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuxsT0z5j2s9"
      },
      "source": [
        "paths = [str(x) for x in Path(\".\").glob(\"text_split/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=8192, min_frequency=2,\n",
        "                show_progress=True,\n",
        "                special_tokens=[\n",
        "                                \"<s>\",\n",
        "                                \"<pad>\",\n",
        "                                \"</s>\",\n",
        "                                \"<unk>\",\n",
        "                                \"<mask>\",\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b13M1o4llUhu",
        "outputId": "16fa11bb-f1b7-4f02-ca52-9581a694765c"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=8192, model=ByteLevelBPE, add_prefix_space=False, lowercase=True, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9grSpVpl6l9",
        "outputId": "5ecff17d-4e8f-4a81-a49a-b64a6f7e4d83"
      },
      "source": [
        "# Tokenizer. save_model () is outdated so we used .save_pretrained()\n",
        "tokenizer.save_model(tokenizer_folder)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Empathy Detection Project/tokenizer/vocab.json',\n",
              " '/content/drive/My Drive/Empathy Detection Project/tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsIqN5LdqXyS"
      },
      "source": [
        "# Create the tokenizer using vocab.json and mrege.txt files\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n",
        "    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hOcwwQztDBZ"
      },
      "source": [
        "# Prepare the tokenizer\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruFAQlf-t2h2",
        "outputId": "1c69b539-1b34-4fad-c6cd-113d0107b86b"
      },
      "source": [
        "tokenizer.encode(\"That's pretty vague, do you not know what you're doing in regards to a specific section of your life? Like school or work?\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=35, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kthKBnPKuWq6",
        "outputId": "bb0c07f5-235a-4e43-b039-5e0d0d9a9540"
      },
      "source": [
        "tokenizer.encode(\"That's pretty vague, do you not know what you're doing in regards to a specific section of your life? Like school or work?\").tokens"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'T',\n",
              " 'hat',\n",
              " \"'s\",\n",
              " 'Ġpretty',\n",
              " 'Ġvague',\n",
              " ',',\n",
              " 'Ġdo',\n",
              " 'Ġyou',\n",
              " 'Ġnot',\n",
              " 'Ġknow',\n",
              " 'Ġwhat',\n",
              " 'Ġyou',\n",
              " \"'re\",\n",
              " 'Ġdoing',\n",
              " 'Ġin',\n",
              " 'Ġregards',\n",
              " 'Ġto',\n",
              " 'Ġa',\n",
              " 'Ġspecific',\n",
              " 'Ġse',\n",
              " 'ction',\n",
              " 'Ġof',\n",
              " 'Ġyour',\n",
              " 'Ġlife',\n",
              " '?',\n",
              " 'Ġ',\n",
              " 'L',\n",
              " 'i',\n",
              " 'ke',\n",
              " 'Ġschool',\n",
              " 'Ġor',\n",
              " 'Ġwork',\n",
              " '?',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyVAQaPYwfOV"
      },
      "source": [
        "# Train Language Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guM09-c1wkki"
      },
      "source": [
        "TRAIN_BATCH_SIZE = 8   # input batch size for training (default: 64)\n",
        "VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n",
        "TRAIN_EPOCHS = 10       # number of epochs to train (default: 10)\n",
        "LEARNING_RATE = 1e-4    # learning rate (default: 0.001)\n",
        "WEIGHT_DECAY = 0.01\n",
        "SEED = 12               # random seed (default: 42)\n",
        "MAX_LEN = 128\n",
        "SUMMARY_LEN = 7"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtTmnpb0xh0h",
        "outputId": "f7c5a860-2487-4189-c142-919831d7d09a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec  4 21:06:06 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GR_nNMFpRUB"
      },
      "source": [
        "# A) Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FLrmdFXzidz",
        "outputId": "4173bf26-8df6-45d3-8a6e-19f3fad7c79f"
      },
      "source": [
        "import tensorflow as tf\n",
        "gpu_available = tf.test.is_gpu_available()\n",
        "print(gpu_available)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-22-be7d3236efc6>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8629qO6i6PhV"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=8192,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tCXTGlM60mr",
        "outputId": "d93c2783-a409-4255-f7c8-b7d308d62cde"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "print('Num parameters: ',model.num_parameters())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num parameters:  49816064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXjtLKSs7QpW"
      },
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "# Create the tokenizer from a trained one\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sSOed9JEMms",
        "outputId": "12131702-4fcc-4c7e-ae2a-e0db29843373"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='/content/drive/My Drive/Empathy Detection Project/tokenizer', vocab_size=8192, model_max_len=128, is_fast=True, padding_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyXzcVf6qbLC"
      },
      "source": [
        "# B) Building the training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS8Vgql7Eh48"
      },
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "numeric_feature_names = ['seeker_post', 'response_post']\n",
        "numeric_features = train_df[numeric_feature_names]\n",
        "dataset = tf.data.experimental.make_csv_dataset(\n",
        "  datafile_path,\n",
        "  batch_size=8,\n",
        "  select_columns = [\"seeker_post\",\"response_post\"])\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        # or use the RobertaTokenizer from `transformers` directly.\n",
        "\n",
        "        self.examples = []\n",
        "        \n",
        "        for example in df.values:\n",
        "            x=tokenizer.encode_plus(example, max_length = MAX_LEN, truncation=True, padding=True)\n",
        "            self.examples += [x.input_ids]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # We’ll pad at the batch level.\n",
        "        return torch.tensor(self.examples[i])\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cX22wzk1vjC"
      },
      "source": [
        "# Create the train and evaluation dataset\n",
        "train_dataset = CustomDataset(train_df['response_post'], tokenizer)\n",
        "eval_dataset = CustomDataset(test_df['response_post'], tokenizer)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SEshDsgsV6z"
      },
      "source": [
        "# C) Define the Data Collactor to Mask Training Data \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohYZvaxl2TQn"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Define the Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1soGaUzkvWnq"
      },
      "source": [
        "# D) Initialize and Run Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5A4CM-Y2hkI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99a71d63-144a-4996-9211-f1f3663f40c9"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "print(model_folder)\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_folder,\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    num_train_epochs=TRAIN_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
        "    save_steps=8192,\n",
        "    #eval_steps=4096,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "# Create the trainer for our model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    #prediction_loss_only=True,\n",
        ")\n",
        "trainer.train()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 3084\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Empathy Detection Project/pre-training model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3860' max='3860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3860/3860 15:45, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.291363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>6.429400</td>\n",
              "      <td>5.943820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.992400</td>\n",
              "      <td>5.810211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.767700</td>\n",
              "      <td>5.658578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>5.767700</td>\n",
              "      <td>5.660955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.696400</td>\n",
              "      <td>5.610998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>5.617500</td>\n",
              "      <td>5.529716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>5.539300</td>\n",
              "      <td>5.466363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>5.539300</td>\n",
              "      <td>5.391001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.494000</td>\n",
              "      <td>5.456916</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3860, training_loss=5.760010809231298, metrics={'train_runtime': 946.0339, 'train_samples_per_second': 32.599, 'train_steps_per_second': 4.08, 'total_flos': 920466171586560.0, 'train_loss': 5.760010809231298, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9YxLg080P5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "1e765eef-e6cb-40b4-810a-f5f1130184bf"
      },
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 233.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtrxHqrJ858O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f48165d-4830-4cc9-954a-bdd625bacf0e"
      },
      "source": [
        "trainer.save_model(model_folder)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/My Drive/Empathy Detection Project/pre-training model\n",
            "Configuration saved in /content/drive/My Drive/Empathy Detection Project/pre-training model/config.json\n",
            "Model weights saved in /content/drive/My Drive/Empathy Detection Project/pre-training model/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YJunFWAvoFl"
      },
      "source": [
        "# Test Trained Model Using a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzR2KyPP9aiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c84668a-39b4-4be2-ff9d-4a04b9f81757"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=model_folder,\n",
        "    tokenizer=tokenizer_folder\n",
        ")\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/My Drive/Empathy Detection Project/pre-training model/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/drive/My Drive/Empathy Detection Project/pre-training model\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8192\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/My Drive/Empathy Detection Project/pre-training model/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/drive/My Drive/Empathy Detection Project/pre-training model\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8192\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/My Drive/Empathy Detection Project/pre-training model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at /content/drive/My Drive/Empathy Detection Project/pre-training model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "Didn't find file /content/drive/My Drive/Empathy Detection Project/tokenizer/added_tokens.json. We won't load it.\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/vocab.json\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/merges.txt\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/special_tokens_map.json\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-Nn3mHr9rOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e434793b-b145-4008-d90b-0160fb24bac9"
      },
      "source": [
        "# Test masked language model\n",
        "fill_mask(\"Yeah, this <mask> is a strong one. Felt it for a lot of my childhood and  adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing ... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (448 > 128). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.0658024474978447,\n",
              "  'sequence': \"Yeah, this, is a strong one. Felt it for a lot of my childhood and  adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 16,\n",
              "  'token_str': ','},\n",
              " {'score': 0.039300814270973206,\n",
              "  'sequence': \"Yeah, this  is a strong one. Felt it for a lot of my childhood and  adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 225,\n",
              "  'token_str': ' '},\n",
              " {'score': 0.03605207800865173,\n",
              "  'sequence': \"Yeah, this to is a strong one. Felt it for a lot of my childhood and  adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 278,\n",
              "  'token_str': ' to'},\n",
              " {'score': 0.031617291271686554,\n",
              "  'sequence': \"Yeah, this you is a strong one. Felt it for a lot of my childhood and  adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 284,\n",
              "  'token_str': ' you'},\n",
              " {'score': 0.030834127217531204,\n",
              "  'sequence': \"Yeah, this. is a strong one. Felt it for a lot of my childhood and  adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 18,\n",
              "  'token_str': '.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}