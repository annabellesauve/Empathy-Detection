{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CISC867 Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYhlssz/qWMEqRWN5G7LuD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annabellesauve/Empathy-Detection/blob/main/CISC867_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8vLLI5pai9i",
        "outputId": "6523307b-5d8e-40e2-f4fb-4b62285f58fb"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 41.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFSxvfY3TST8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from transformers import RobertaTokenizer, TFRobertaModel, TFRobertaForMaskedLM\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qvu-Y1khPOT"
      },
      "source": [
        "Roberta documentation: https://huggingface.co/transformers/model_doc/roberta.html?highlight=roberta#tfrobertamodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnh08SdiTNuz"
      },
      "source": [
        "# Encoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSXFyPK6VE0y",
        "outputId": "745f58d2-d242-41fe-b5a8-e33c5f749d7f"
      },
      "source": [
        "# robertabase model initialization, from huggingface\n",
        "robertabase_tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "robertabase_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "robertabase_maskedLM_model = TFRobertaForMaskedLM.from_pretrained('roberta-base')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
            "\n",
            "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4YhNHXlUSD8"
      },
      "source": [
        "**S Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HnNhFP-UHyc"
      },
      "source": [
        "# def S_Encoder(seeker_post, id_SP, attention_SP):\n",
        "class S_Encoder(tf.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  # def _init_weights(self, module):\n",
        "  #   # don't know what weights to use to initialize.. not much info \n",
        "  #   module.bias.data.zero_()\n",
        "  #   module.weight.data.fill_(1.0)\n",
        "  \n",
        "  def encode(self, seeker_post):\n",
        "    encoded_input = robertabase_tokenizer(seeker_post, return_tensors='tf')\n",
        "    return robertabase_model(encoded_input)\n",
        "    # input_ids = np.array(robertabase_tokenizer.encode(seeker_post, add_special_tokens=True))\n",
        "    # return robertabase_maskedLM_model(input_ids, labels=input_ids)\n",
        "    # inputs = robertabase_tokenizer(seeker_post_mask, return_tensors=\"tf\")\n",
        "    # inputs[\"labels\"] = robertabase_tokenizer(seeker_post, return_tensors=\"tf\")[\"input_ids\"]\n",
        "    # return robertabase_maskedLM_model(inputs)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8osFmUMfUYAu"
      },
      "source": [
        "**R Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TfyIoGdUo4J"
      },
      "source": [
        "class R_Encoder(tf.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  # def _init_weights(self, module):\n",
        "  #   # don't know what weights to use to initialize.. not much info \n",
        "  #   module.bias.data.zero_()\n",
        "  #   module.weight.data.fill_(1.0)\n",
        "\n",
        "  def encode(self, response_post):\n",
        "    encoded_input = robertabase_tokenizer(response_post, return_tensors='tf')\n",
        "    return robertabase_model(encoded_input)\n",
        "\n",
        "  # def encode_mask(response_post, response_post_mask):\n",
        "\n",
        "  #   inputs = robertabase_tokenizer(response_post_mask, return_tensors=\"tf\")[\"input_ids\"]\n",
        "  #   inputs[\"labels\"] = robertabase_tokenizer(response_post, return_tensors=\"tf\")[\"input_ids\"]\n",
        "  #   return robertabase_maskedLM_model(inputs)\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpPvfISMXz3v"
      },
      "source": [
        "# Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZPUBIT5py59"
      },
      "source": [
        "class AttentionLayer(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(AttentionLayer, self).__init__()\n",
        "\n",
        "  def call(self, e_R, e_S, d):\n",
        "    a = tf.math.multiply(tf.nn.softmax(tf.math.divide(tf.math.multiply(e_R, e_S), math.sqrt(d))), e_R)\n",
        "    return a\n",
        "\n",
        "\n",
        "\n",
        "## dont know where this is used   \n",
        "def residual(e_R, e_S, d):\n",
        "  h = e_R + attention(e_R, e_S, d)\n",
        "  return h"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX96QMmWuaMc"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZstaEEzuZx8"
      },
      "source": [
        "# BiEncoderAttentionWithRationaleClassification() in models.py\n",
        "class Model(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, hidden_dropout_prob=0.2, rationale_num_labels=2, empathy_num_labels= 3, hidden_size=768):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.rationale_classifier = tf.keras.layers.Dense(rationale_num_labels, activation=None) # linear layer\n",
        "    self.attention = AttentionLayer()\n",
        "    self.rationale_num_labels = rationale_num_labels\n",
        "    self.empathy_num_labels = empathy_num_labels\n",
        "    self.empathy_classifier = EmpathyClassifier() # linear layer\n",
        "    self.r_encoder = R_Encoder()\n",
        "    self.s_encoder = S_Encoder()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  # def _init_weights(self, module):\n",
        "  #   # don't know what weights to use to initialize.. not much info \n",
        "  #   module.bias.data.zero_()\n",
        "  #   module.weight.data.fill_(1.0)\n",
        "  \n",
        "  def call(self,\n",
        "\t\tinput_ids_SP=None,\n",
        "\t\tinput_ids_RP=None,\n",
        "\t\tattention_mask_SP=None,\n",
        "\t\tattention_mask_RP=None,\n",
        "\t\ttoken_type_ids_SP=None,\n",
        "\t\ttoken_type_ids_RP=None,\n",
        "\t\tposition_ids_SP=None,\n",
        "\t\tposition_ids_RP=None,\n",
        "\t\thead_mask_SP=None,\n",
        "\t\thead_mask_RP=None,\n",
        "\t\tinputs_embeds_SP=None,\n",
        "\t\tinputs_embeds_RP=None,\n",
        "\t\tempathy_labels=None,\n",
        "\t\trationale_labels=None,\n",
        "\t\tlambda_EI=1,\n",
        "\t\tlambda_RE=0.1,\n",
        "    SP=None,\n",
        "    RP=None\n",
        "    ):\n",
        "\n",
        "    output_SP = []\n",
        "    output_RP = []\n",
        "\n",
        "    for post in SP:\n",
        "      output_SP.append(self.s_encoder.encode(post))\n",
        "    \n",
        "    for post in RP:\n",
        "      output_RP.append(self.r_encoder.encode(post))\n",
        "\n",
        "    sequence_output_SP = output_SP[0].pooler_output\n",
        "    sequence_output_RP = output_RP[0].pooler_output\n",
        "\n",
        "    attn = self.attention(sequence_output_RP, sequence_output_SP, self.hidden_size)\n",
        "    sequence_output_RP = sequence_output_RP + tf.nn.dropout(attn, self.hidden_dropout_prob)\n",
        "\n",
        "    logits_empathy = self.empathy_classifier(sequence_output_RP)\n",
        "\n",
        "    sequence_output = tf.nn.dropout(sequence_output_RP, self.hidden_dropout_prob)\n",
        "    logits_rationales = self.rationale_classifier(sequence_output)\n",
        "\n",
        "    labels = tf.reshape(empathy_labels, -1)\n",
        "    logits = tf.reshape(logits_empathy, (-1, self.empathy_num_labels))\n",
        "    loss_empathy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits).numpy()\n",
        "\n",
        "    print(\"loss empathy:\", loss_empathy)\n",
        "\n",
        "    active_loss = tf.reshape(attention_mask_RP, -1)\n",
        "    active_logits = tf.reshape(logits_rationales, (-1, self.rationale_num_labels))\n",
        "\n",
        "    if 1 in active_loss: # not sure about this condition, they use == 1\n",
        "      # active_labels = tf.reshape(rationale_labels, -1)\n",
        "      active_labels = tf.reshape([1], -1)\n",
        "    else:\n",
        "      # active_labels = tf.tensor(-100, shape=-1, dtype=tf.dtype(rationale_labels))\n",
        "      active_labels = tf.reshape([0], -1)\n",
        "\n",
        "    loss_rationales = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=active_labels, logits=active_logits).numpy()\n",
        "\n",
        "    print(\"loss rationale:\", loss_rationales)\n",
        "\n",
        "    overall_loss = lambda_EI * loss_empathy + lambda_RE + loss_rationales\n",
        "\n",
        "    return (overall_loss, loss_empathy, loss_rationales, logits_empathy, logits_rationales)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RULsqYSOsGDs"
      },
      "source": [
        "# Empathy Identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXn16w96sQRG"
      },
      "source": [
        "# RobertaClassificationHead in model.py\n",
        "class EmpathyClassifier(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, hidden_dropout_prob=0.1, hidden_size=768, empathy_num_labels=3):\n",
        "    super(EmpathyClassifier, self).__init__()\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(hidden_size, activation=None)\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.out_proj = tf.keras.layers.Dense(empathy_num_labels, activation=None)\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "\n",
        "  def call(self, features):\n",
        "    x = features[:, :]\n",
        "    x = tf.nn.dropout(x, self.hidden_dropout_prob)\n",
        "    x = self.dense(x)\n",
        "    x = self.relu(x)\n",
        "    x = tf.nn.dropout(x, self.hidden_dropout_prob)\n",
        "    x = self.out_proj(x)\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEN6n86d7Pjj"
      },
      "source": [
        "# Initializing model + training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiWFQmucaU6s",
        "outputId": "a8aa3e77-5c5e-4fb5-85b2-56699afa8bbb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPMI-feNa6kF"
      },
      "source": [
        "# load dataset\n",
        "  # assuming for now seeker posts is \"seeker_posts\" and reponse posts is \"response_posts\"\n",
        "df_ER = pd.read_csv('/content/drive/My Drive/emotionalreactions.csv')\n",
        "df_ER = df_ER.head(50) # temporary\n",
        "\n",
        "df_ER['response_post_masked'] = df_ER['response_post_masked'].replace(np.nan, 0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN-XIafgcEdn"
      },
      "source": [
        "# roberta domain adaptive pre training 3 epoch 8 batch\n",
        "\n",
        "# can't get this to work. the tfrobertaformaskedlm example doesn't even work.\n",
        "\n",
        "# optimizer_roberta = tfa.optimizers.AdamW(\n",
        "#     weight_decay=1e-2, # not given but default in py torch so probably used\n",
        "#     learning_rate=2e-5, # given\n",
        "#     epsilon=1e-8 # given\n",
        "# )\n",
        "\n",
        "# sp = df_ER.seeker_post \n",
        "# rp = df_ER.response_post\n",
        "# rp_masked = df_ER.response_post_masked\n",
        "\n",
        "# encoder_train_data = [sp, rp, rp_masked]\n",
        "\n",
        "# S_encoder_model = S_Encoder()\n",
        "# R_encoder_model = R_Encoder()\n",
        "\n",
        "# total_encoder_train_loss = 0\n",
        "# total_encoder_sp_loss = 0\n",
        "# total_encoder_rp_loss = 0\n",
        "\n",
        "# for epoch in range(0, 3):\n",
        "#   print(\"Training epoch #\", epoch, \"...\")\n",
        "\n",
        "#   for i in range(len(encoder_train_data[0])):\n",
        "#       e_sp = encoder_train_data[0][i] # seeker post \n",
        "#       e_rp = encoder_train_data[1][i] # response post \n",
        "#       e_rp_masked = encoder_train_data[2][i] # masked response post\n",
        "\n",
        "#       # sp_output = S_Encoder.encode(e_sp)\n",
        "#       if not isinstance(e_rp_masked, str):\n",
        "#         continue \n",
        "#       else:\n",
        "#         rp_output = R_Encoder.encode_mask(e_rp, e_rp_masked)\n",
        "\n",
        "#         print(rp_output)\n",
        "#         total_encoder_train_loss += rp_output.loss # what is total loss? no info\n",
        "#         # total_encoder_sp_loss += sp_output.loss\n",
        "#         total_encoder_rp_loss += rp_output.loss\n",
        "\n",
        "#         optimizer_roberta.minimize(rp_output.loss) # update optimizer\n",
        "\n",
        "#         print(rp_output.loss)\n",
        "    \n",
        "#   print(\"Epoch #\", epoch)\n",
        "#   print(\"Total loss:\", total_train_loss)\n",
        "#   print(\"Total empathy loss:\", total_train_empathy_loss)\n",
        "#   print(\"Total rationale loss:\", total_train_rationale_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB1PbYtl7V_o"
      },
      "source": [
        "# tokenize input \n",
        "tokenizer_RP = robertabase_tokenizer.batch_encode_plus(df_ER.response_post, add_special_tokens=True, max_length=64, padding='max_length', truncation=True)\n",
        "input_ids_RP = tokenizer_RP['input_ids']\n",
        "attention_masks_RP = tokenizer_RP['attention_mask']\n",
        "\n",
        "tokenizer_SP = robertabase_tokenizer.batch_encode_plus(df_ER.seeker_post, add_special_tokens=True, max_length=64, padding='max_length', truncation=True)\n",
        "input_ids_SP = tokenizer_SP['input_ids']\n",
        "attention_masks_SP = tokenizer_RP['attention_mask']\n",
        "\n",
        "labels = np.array(df_ER.level.values.astype(int))\n",
        "rationales = df_ER.rationale_labels.values\n",
        "\n",
        "ER_model = Model()\n",
        "# IN_model = Model()\n",
        "# EX_model = Model()\n",
        "\n",
        "optimizer_model = tfa.optimizers.AdamW(\n",
        "    weight_decay=1e-2, # not given but default in py torch so probably used\n",
        "    learning_rate=2e-5, # given\n",
        "    epsilon=1e-8 # given\n",
        ")\n",
        "\n",
        "# ER_model.compile(optimizer=optimizer_model)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbyGRCtYivp3"
      },
      "source": [
        "# train_dataset = tf.data.Dataset(input_ids_SP, attention_masks_SP, input_ids_RP, attention_masks_RP, labels, rationales)\n",
        "train_dataset = [input_ids_SP, attention_masks_SP, input_ids_RP, attention_masks_RP, labels, rationales]\n",
        "\n",
        "# print(labels)\n",
        "train_size = int(len(train_dataset))\n",
        "\n",
        "total_steps = train_size * 4 # 4 is epoch\n",
        "# num_batch = int(train_size / 32) # 32 is batch size\n",
        "\n",
        "tf.random.set_seed(12) # given seed"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "2SqYT52kkPSS",
        "outputId": "aff9ce80-d4e7-4e43-9c04-207efed1ce41"
      },
      "source": [
        "for epoch in range(0, 4):\n",
        "  print(\"Training epoch #\", epoch, \"...\")\n",
        "  total_train_loss = 0\n",
        "  total_train_empathy_loss = 0\n",
        "  total_train_rationale_loss = 0\n",
        "\n",
        "  # ER_model.train()\n",
        "\n",
        "  for i in range(len(train_dataset[0])):\n",
        "  # for element in train_dataset:\n",
        "      e_id_SP = train_dataset[0][i] # get seeker id \n",
        "      e_attention_SP = train_dataset[1][i] # get seeker attention \n",
        "      e_id_RP = train_dataset[2][i] # get response id \n",
        "      e_attention_RP = train_dataset[3][i] # get response attention\n",
        "      e_labels = train_dataset[4][i] # get labels\n",
        "      # e_rationales = train_dataset[5][i]\n",
        "      e_rationales = list(map(int, train_dataset[5][i].split(','))) # get rationales, transform to array of ints\n",
        "\n",
        "      loss, loss_empathy, loss_rationale, logits_empathy, logits_rationale = ER_model(\n",
        "          input_ids_SP=e_id_SP,\n",
        "          input_ids_RP=e_id_RP,\n",
        "          attention_mask_SP=e_attention_SP,\n",
        "          attention_mask_RP=e_attention_RP,\n",
        "          empathy_labels=e_labels,\n",
        "          rationale_labels=e_rationales,\n",
        "          SP=df_ER.seeker_post,\n",
        "          RP=df_ER.response_post\n",
        "          )\n",
        "      \n",
        "      total_train_loss += loss\n",
        "      total_train_empathy_loss += loss_empathy\n",
        "      total_train_rationale_loss += loss_rationale\n",
        "\n",
        "      # print(ER_model.trainable_weights)\n",
        "\n",
        "      optimizer_model.minimize(loss, var_list=ER_model.trainable_weights, tape=tf.GradientTape(persistent=False)) # update optimizer\n",
        "      # getting 'numpy.ndarray' object has no attribute '_id' error, has to do with tape.\n",
        "      # if remove tape then error also\n",
        "\n",
        "      print(\"loss:\", loss)\n",
        "    \n",
        "  print(\"Epoch #\", epoch)\n",
        "  print(\"Total loss:\", total_train_loss)\n",
        "  print(\"Total empathy loss:\", total_train_empathy_loss)\n",
        "  print(\"Total rationale loss:\", total_train_rationale_loss)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch # 0 ...\n",
            "loss empathy: [0.8937717]\n",
            "loss rationale: [0.7969197]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-79552519819f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m# print(ER_model.trainable_weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0moptimizer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_empathy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_rationale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, decay_var_list, tape)\u001b[0m\n\u001b[1;32m    144\u001b[0m         )\n\u001b[1;32m    145\u001b[0m         return super().minimize(\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \"\"\"\n\u001b[1;32m    520\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 521\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     self._assert_valid_dtypes([\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_id'"
          ]
        }
      ]
    }
  ]
}