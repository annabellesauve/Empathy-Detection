{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xwbACjmvfJRN",
        "Rnh08SdiTNuz",
        "bpPvfISMXz3v",
        "dX96QMmWuaMc",
        "RULsqYSOsGDs",
        "Tuq8V76eNPLi",
        "MEN6n86d7Pjj",
        "bP1w6Pw3UWgt",
        "Dw0jZPXxUdOg"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMOzLhACz7dPITsljVlMFrA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e92e8ba1a26e4ef684bfa55213fcaff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_62ef13565ed04759a15dfb364c641d3b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_868593a0a8b54e45a836acff1476bed8",
              "IPY_MODEL_cafc5768c3b646bea8ac91c0bd99b727",
              "IPY_MODEL_decbe74ab1c941dba33583f4b6303d4c"
            ]
          }
        },
        "62ef13565ed04759a15dfb364c641d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "868593a0a8b54e45a836acff1476bed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aebc2dd0a4964671a2b82bc71c1d4687",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_339cf5aec0bb4319a6eee750c747e36e"
          }
        },
        "cafc5768c3b646bea8ac91c0bd99b727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_67f97c6aac694533b0bbd5f2ac6ea6f6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d71e226c8558414eb7fab3aec2cac0c3"
          }
        },
        "decbe74ab1c941dba33583f4b6303d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a7ed01dffc24c088e05121995739720",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 878k/878k [00:00&lt;00:00, 2.79MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_498fadc27bfd42ecb10c71f8739b0124"
          }
        },
        "aebc2dd0a4964671a2b82bc71c1d4687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "339cf5aec0bb4319a6eee750c747e36e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67f97c6aac694533b0bbd5f2ac6ea6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d71e226c8558414eb7fab3aec2cac0c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a7ed01dffc24c088e05121995739720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "498fadc27bfd42ecb10c71f8739b0124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be8a55871ee94315a36866cb19922956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_275d406fed66402ea473c9c92295d7b4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_20004e73f304455c8fea3a3c4daf22eb",
              "IPY_MODEL_c0f9191812574874bd6d0bea369d720e",
              "IPY_MODEL_8be2ea459ef84ed9aa2d8a833e7371e2"
            ]
          }
        },
        "275d406fed66402ea473c9c92295d7b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20004e73f304455c8fea3a3c4daf22eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d17e6c1318cf41fdbe960277bac13f4e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_59e8d806698d40a09efc887bf2ba714d"
          }
        },
        "c0f9191812574874bd6d0bea369d720e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4bb53c7d5d654b54a02bc12c2c571f2e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1bcf7bcaafe4778b59e3fafd0a9c644"
          }
        },
        "8be2ea459ef84ed9aa2d8a833e7371e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_63fe3d7eaa7c4e6699a8299e694f0f26",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:00&lt;00:00, 858kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a301cd88ff6496e8288891738678034"
          }
        },
        "d17e6c1318cf41fdbe960277bac13f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "59e8d806698d40a09efc887bf2ba714d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4bb53c7d5d654b54a02bc12c2c571f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1bcf7bcaafe4778b59e3fafd0a9c644": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63fe3d7eaa7c4e6699a8299e694f0f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a301cd88ff6496e8288891738678034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95eac52d72d547599d33424710221bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d68a512bdec6489c971a9d6b3aaa8b11",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c5095d3e2e3844c89531694e58ecfa08",
              "IPY_MODEL_9c4e3b997754478698ddc75fdf924127",
              "IPY_MODEL_528b2d224a2641c0ab132abf7be09d31"
            ]
          }
        },
        "d68a512bdec6489c971a9d6b3aaa8b11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5095d3e2e3844c89531694e58ecfa08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_65e463547c934b75a4a0a7c45a8b1ac0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aefa9607a4354623a0cd79a0efb9b093"
          }
        },
        "9c4e3b997754478698ddc75fdf924127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3c51fc334fdd44498b4b5268631a841c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0d16036645b4e8083cb585554b9dc21"
          }
        },
        "528b2d224a2641c0ab132abf7be09d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_94d9a986575345b79990d57c1efd5f0c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.29M/1.29M [00:00&lt;00:00, 4.07MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_728b2a04066e42928589987872d4e0b3"
          }
        },
        "65e463547c934b75a4a0a7c45a8b1ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aefa9607a4354623a0cd79a0efb9b093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c51fc334fdd44498b4b5268631a841c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0d16036645b4e8083cb585554b9dc21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94d9a986575345b79990d57c1efd5f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "728b2a04066e42928589987872d4e0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59d14a847cc2464cb63537c2825f7669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_23183a2beb8f41858a793ab84cc5516f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4526a1c6fe7640088f5ff9eaa9a68847",
              "IPY_MODEL_2f82a351d5b44cc99fa8975ebaacbc67",
              "IPY_MODEL_df3135f4db3845ec80a91f5da9e6c0dc"
            ]
          }
        },
        "23183a2beb8f41858a793ab84cc5516f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4526a1c6fe7640088f5ff9eaa9a68847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0e32240b3fab4e9c8498d60984f49f8c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e86c711db7e423dbbf33b95fa0ab851"
          }
        },
        "2f82a351d5b44cc99fa8975ebaacbc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dda14e73e2c142339cdda453fe96f203",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec783bb5e9dd4d3089ef857f109cc337"
          }
        },
        "df3135f4db3845ec80a91f5da9e6c0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_42d509b2e9544f4aa7f808d4e2bf781f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 17.1kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6f4e0c86a154da0bb02cb12819d7827"
          }
        },
        "0e32240b3fab4e9c8498d60984f49f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e86c711db7e423dbbf33b95fa0ab851": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dda14e73e2c142339cdda453fe96f203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec783bb5e9dd4d3089ef857f109cc337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42d509b2e9544f4aa7f808d4e2bf781f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6f4e0c86a154da0bb02cb12819d7827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b79d9188c944a7fb3feb731316f8975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5f52575b8ec44128b34a4658233f4124",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c1e7f293584440ea826a278877ff9cc9",
              "IPY_MODEL_c3117da64af949d2a2e2510a7a3bee16",
              "IPY_MODEL_31a606f4409d4435919274a969d1cc87"
            ]
          }
        },
        "5f52575b8ec44128b34a4658233f4124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1e7f293584440ea826a278877ff9cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cdfa2638645241b587e3a51f8275c58d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf1439f7a59c4430965402f143cb8ee2"
          }
        },
        "c3117da64af949d2a2e2510a7a3bee16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3177f45289a74f35a68c8166c26d3ae2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 657434796,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 657434796,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4af847ef8319437588914f6ceaffe416"
          }
        },
        "31a606f4409d4435919274a969d1cc87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5952ac1888a844ae93f712d6f23b49b4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 627M/627M [00:16&lt;00:00, 42.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6520504fca2844049f904c3d09f2e3f2"
          }
        },
        "cdfa2638645241b587e3a51f8275c58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf1439f7a59c4430965402f143cb8ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3177f45289a74f35a68c8166c26d3ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4af847ef8319437588914f6ceaffe416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5952ac1888a844ae93f712d6f23b49b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6520504fca2844049f904c3d09f2e3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annabellesauve/Empathy-Detection/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8vLLI5pai9i",
        "outputId": "f91e8a36-d012-43fc-c5e0-3fd628a0d742"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 22.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrjIrIMYg1cc"
      },
      "source": [
        "# need if running locally\n",
        "# import os\n",
        "# os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5/bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C13xH9b6SMV_"
      },
      "source": [
        "# paste this code in dev tool console to avoid disconnecting from the colab runtime\n",
        "# function ClickConnect(){\n",
        "# console.log(\"Working\"); \n",
        "# document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "# }\n",
        "# setInterval(ClickConnect,60000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFSxvfY3TST8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from transformers import RobertaTokenizer, TFRobertaModel, RobertaForMaskedLM, RobertaConfig, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm\n",
        "import math\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adyBswtmA4wI"
      },
      "source": [
        "# Data Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZZ0n93WDF7Q",
        "outputId": "7d42b16b-5b01-4c6d-879a-b92564c4d85c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPMI-feNa6kF"
      },
      "source": [
        "# load datasets directly from our repo\n",
        "\n",
        "df_ER = pd.read_csv('https://raw.githubusercontent.com/annabellesauve/Empathy-Detection/main/Datasets/processed_emotionalreactions.csv')\n",
        "ER_train, ER_test = train_test_split(df_ER, test_size=0.2)\n",
        "\n",
        "df_EX = pd.read_csv('https://raw.githubusercontent.com/annabellesauve/Empathy-Detection/main/Datasets/processed_explorations-reddit.csv')\n",
        "EX_train, EX_test = train_test_split(df_EX, test_size=0.2)\n",
        "\n",
        "df_IN = pd.read_csv('https://raw.githubusercontent.com/annabellesauve/Empathy-Detection/main/Datasets/processed_interpretations-reddit.csv')\n",
        "IN_train, IN_test = train_test_split(df_IN, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwbACjmvfJRN"
      },
      "source": [
        "# Domain Adaptive Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifhh8DGef3qs"
      },
      "source": [
        "# model_folder = '/content/drive/My Drive/empathy/pre_training_model'\n",
        "# tokenizer_folder = '/content/drive/My Drive/empathy/tokenizer'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW5ap-tugYLT"
      },
      "source": [
        "# df_pretrain = pd.read_csv('https://raw.githubusercontent.com/annabellesauve/Empathy-Detection/main/Datasets/processed_emotionalreactions.csv', header=0, usecols=[1, 2])\n",
        "# pretrain_train, pretrain_test = train_test_split(df_pretrain, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXHPzqiIi4Vs"
      },
      "source": [
        "# PRETRAIN_BATCH = 8\n",
        "# PRETRAIN_EPOCHS = 3\n",
        "# MAX_LEN = 128\n",
        "\n",
        "# config = RobertaConfig(\n",
        "#     vocab_size=8192,\n",
        "#     max_position_embeddings=514,\n",
        "#     num_attention_heads=12,\n",
        "#     num_hidden_layers=6,\n",
        "#     type_vocab_size=1,\n",
        "# )\n",
        "\n",
        "# robertabase_maskedLM_model = RobertaForMaskedLM(config=config)\n",
        "# robertabase_tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrEvlJdWkBL-"
      },
      "source": [
        "# from torch.utils.data.dataset import Dataset\n",
        "# import torch\n",
        "\n",
        "# class CustomDS(Dataset):\n",
        "#   def __init__(self, df):\n",
        "#     self.examples = []\n",
        "        \n",
        "#     for example in df.values:\n",
        "#         x = robertabase_tokenizer.encode_plus(example, max_length = MAX_LEN, truncation=True, padding=True)\n",
        "#         self.examples += [x.input_ids]\n",
        "\n",
        "#   def __len__(self):\n",
        "#       return len(self.examples)\n",
        "\n",
        "#   def __getitem__(self, i):\n",
        "#       return torch.tensor(self.examples[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uro47YpykX2w"
      },
      "source": [
        "# pretrain_train_DS = CustomDS(pretrain_train['response_post'])\n",
        "# pretrain_eval_DS = CustomDS(pretrain_test['response_post'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkVqdGb_kuWk"
      },
      "source": [
        "# data_collator = DataCollatorForLanguageModeling(\n",
        "#     tokenizer=robertabase_tokenizer, mlm=True, mlm_probability=0.15\n",
        "# )\n",
        "\n",
        "# # Define the training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=model_folder,\n",
        "#     overwrite_output_dir=True,\n",
        "#     evaluation_strategy = 'epoch',\n",
        "#     num_train_epochs=PRETRAIN_EPOCHS,\n",
        "#     per_device_train_batch_size=PRETRAIN_BATCH,\n",
        "#     save_steps=8192,\n",
        "#     #eval_steps=4096,\n",
        "#     save_total_limit=1,\n",
        "# )\n",
        "# # Create the trainer for our model\n",
        "# trainer = Trainer(\n",
        "#     model=robertabase_maskedLM_model,\n",
        "#     args=training_args,\n",
        "#     data_collator=data_collator,\n",
        "#     train_dataset=pretrain_train_DS,\n",
        "#     eval_dataset=pretrain_eval_DS,\n",
        "#     #prediction_loss_only=True,\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbb50WTylY05"
      },
      "source": [
        "# trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKeaOib1m3Da"
      },
      "source": [
        "# trainer.save_model(model_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnh08SdiTNuz"
      },
      "source": [
        "# Encoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e92e8ba1a26e4ef684bfa55213fcaff1",
            "62ef13565ed04759a15dfb364c641d3b",
            "868593a0a8b54e45a836acff1476bed8",
            "cafc5768c3b646bea8ac91c0bd99b727",
            "decbe74ab1c941dba33583f4b6303d4c",
            "aebc2dd0a4964671a2b82bc71c1d4687",
            "339cf5aec0bb4319a6eee750c747e36e",
            "67f97c6aac694533b0bbd5f2ac6ea6f6",
            "d71e226c8558414eb7fab3aec2cac0c3",
            "2a7ed01dffc24c088e05121995739720",
            "498fadc27bfd42ecb10c71f8739b0124",
            "be8a55871ee94315a36866cb19922956",
            "275d406fed66402ea473c9c92295d7b4",
            "20004e73f304455c8fea3a3c4daf22eb",
            "c0f9191812574874bd6d0bea369d720e",
            "8be2ea459ef84ed9aa2d8a833e7371e2",
            "d17e6c1318cf41fdbe960277bac13f4e",
            "59e8d806698d40a09efc887bf2ba714d",
            "4bb53c7d5d654b54a02bc12c2c571f2e",
            "b1bcf7bcaafe4778b59e3fafd0a9c644",
            "63fe3d7eaa7c4e6699a8299e694f0f26",
            "9a301cd88ff6496e8288891738678034",
            "95eac52d72d547599d33424710221bc1",
            "d68a512bdec6489c971a9d6b3aaa8b11",
            "c5095d3e2e3844c89531694e58ecfa08",
            "9c4e3b997754478698ddc75fdf924127",
            "528b2d224a2641c0ab132abf7be09d31",
            "65e463547c934b75a4a0a7c45a8b1ac0",
            "aefa9607a4354623a0cd79a0efb9b093",
            "3c51fc334fdd44498b4b5268631a841c",
            "e0d16036645b4e8083cb585554b9dc21",
            "94d9a986575345b79990d57c1efd5f0c",
            "728b2a04066e42928589987872d4e0b3",
            "59d14a847cc2464cb63537c2825f7669",
            "23183a2beb8f41858a793ab84cc5516f",
            "4526a1c6fe7640088f5ff9eaa9a68847",
            "2f82a351d5b44cc99fa8975ebaacbc67",
            "df3135f4db3845ec80a91f5da9e6c0dc",
            "0e32240b3fab4e9c8498d60984f49f8c",
            "1e86c711db7e423dbbf33b95fa0ab851",
            "dda14e73e2c142339cdda453fe96f203",
            "ec783bb5e9dd4d3089ef857f109cc337",
            "42d509b2e9544f4aa7f808d4e2bf781f",
            "b6f4e0c86a154da0bb02cb12819d7827",
            "6b79d9188c944a7fb3feb731316f8975",
            "5f52575b8ec44128b34a4658233f4124",
            "c1e7f293584440ea826a278877ff9cc9",
            "c3117da64af949d2a2e2510a7a3bee16",
            "31a606f4409d4435919274a969d1cc87",
            "cdfa2638645241b587e3a51f8275c58d",
            "cf1439f7a59c4430965402f143cb8ee2",
            "3177f45289a74f35a68c8166c26d3ae2",
            "4af847ef8319437588914f6ceaffe416",
            "5952ac1888a844ae93f712d6f23b49b4",
            "6520504fca2844049f904c3d09f2e3f2"
          ]
        },
        "id": "HSXFyPK6VE0y",
        "outputId": "d23d3085-e867-4462-bc93-5dbc70572bbf"
      },
      "source": [
        "# robertabase model initialization, from huggingface\n",
        "robertabase_tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "robertabase_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "\n",
        "# robertabase_model = RobertaForMaskedLM.from_pretrained(model_folder) # trying to load model that was pretrained - does not work since it's a masked language task model and the task in the encoder is not a masked language modeling task.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e92e8ba1a26e4ef684bfa55213fcaff1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be8a55871ee94315a36866cb19922956",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95eac52d72d547599d33424710221bc1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59d14a847cc2464cb63537c2825f7669",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b79d9188c944a7fb3feb731316f8975",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/627M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qvu-Y1khPOT"
      },
      "source": [
        "Roberta documentation: https://huggingface.co/transformers/model_doc/roberta.html?highlight=roberta#tfrobertamodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4YhNHXlUSD8"
      },
      "source": [
        "**S Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HnNhFP-UHyc"
      },
      "source": [
        "class S_Encoder(tf.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def encode(self, seeker_post):\n",
        "    seeker_post = seeker_post[:512]\n",
        "    encoded_input = robertabase_tokenizer(seeker_post, return_tensors='tf')\n",
        "    return robertabase_model(encoded_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8osFmUMfUYAu"
      },
      "source": [
        "**R Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TfyIoGdUo4J"
      },
      "source": [
        "class R_Encoder(tf.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def encode(self, response_post):\n",
        "    response_post = response_post[:512]\n",
        "    encoded_input = robertabase_tokenizer(response_post, return_tensors='tf')\n",
        "    return robertabase_model(encoded_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpPvfISMXz3v"
      },
      "source": [
        "# Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZPUBIT5py59"
      },
      "source": [
        "# single head attention layer\n",
        "class AttentionLayer(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(AttentionLayer, self).__init__()\n",
        "\n",
        "  def call(self, e_R, e_S, d):\n",
        "    a = tf.math.multiply(tf.nn.softmax(tf.math.divide(tf.math.multiply(e_R, e_S), math.sqrt(d))), e_R)\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX96QMmWuaMc"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZstaEEzuZx8"
      },
      "source": [
        "# add custom metrics\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "empathy_metric = tf.keras.metrics.Mean(name=\"empathy loss\")\n",
        "rationale_metric = tf.keras.metrics.Mean(name=\"rationale loss\")\n",
        "empathy_accuracy = tf.keras.metrics.Accuracy(name=\"empathy accuracy\")\n",
        "rationale_accuracy = tf.keras.metrics.Accuracy(name=\"rationale accuracy\")\n",
        "f1_rationale = tf.keras.metrics.Mean(name=\"rationale f1\")\n",
        "\n",
        "# define f1 function for rationale \n",
        "def f1(y_true, y_pred):  \n",
        "  tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # true positive\n",
        "  pos = K.sum(K.round(K.clip(y_true, 0, 1))) # possible positive\n",
        "  pred = K.sum(K.round(K.clip(y_pred, 0, 1)))  # predicted positive\n",
        "\n",
        "  pos = tf.add(tf.cast(pos, tf.float32), K.epsilon())\n",
        "  pred = tf.add(tf.cast(pred, tf.float32), K.epsilon())\n",
        "  \n",
        "  recall = tf.cast(tp, tf.float32) / (pos)\n",
        "  precision = tf.cast(tp, tf.float32) / (pred)\n",
        "\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "# modeled after BiEncoderAttentionWithRationaleClassification() in original models.py\n",
        "class Model(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, hidden_dropout_prob=0.2, rationale_num_labels=2, empathy_num_labels= 3, hidden_size=768):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.rationale_classifier = tf.keras.layers.Dense(rationale_num_labels, activation=None) # linear layer\n",
        "    self.attention = AttentionLayer()\n",
        "    self.rationale_num_labels = rationale_num_labels\n",
        "    self.empathy_num_labels = empathy_num_labels\n",
        "    self.empathy_classifier = EmpathyClassifier() # linear layer\n",
        "    self.r_encoder = R_Encoder()\n",
        "    self.s_encoder = S_Encoder()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.lambda_EI = 1 \n",
        "    self.lambda_RE = 0.1\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    input_ids_SP = inputs[0] # get seeker id \n",
        "    attention_mask_SP = inputs[1] # get seeker attention \n",
        "    input_ids_RP = inputs[2] # get response id \n",
        "    attention_mask_RP = inputs[3] # get response attention\n",
        "    SP = inputs[4] # get seeker post\n",
        "    RP = inputs[5] # get response post\n",
        "\n",
        "    output_RP = []\n",
        "    \n",
        "    # convert to numpy array so we can get the correct string format\n",
        "    SP = SP.numpy()\n",
        "    RP = RP.numpy()\n",
        "\n",
        "    # encode with s/r encoders, pass through attention layer\n",
        "    for i in range(len(SP)):\n",
        "      sp = self.s_encoder.encode(SP[i].decode(\"utf-8\"))\n",
        "      rp = self.r_encoder.encode(RP[i].decode(\"utf-8\"))\n",
        "      attn = self.attention(rp.pooler_output, sp.pooler_output, self.hidden_size)\n",
        "      output_RP.append(rp.pooler_output + tf.nn.dropout(attn, self.hidden_dropout_prob))\n",
        "\n",
        "    output_RP_tensor = tf.convert_to_tensor(output_RP, dtype=tf.float32)\n",
        "\n",
        "    # empathy classification\n",
        "    logits_empathy = self.empathy_classifier(output_RP_tensor)\n",
        "\n",
        "    # dropout layer\n",
        "    sequence_output = tf.nn.dropout(output_RP_tensor, self.hidden_dropout_prob)\n",
        "\n",
        "    # rationale classification\n",
        "    logits_rationale = self.rationale_classifier(sequence_output)\n",
        "\n",
        "    return (logits_empathy, logits_rationale) # outputs\n",
        "  \n",
        "  def train_step(self, data): # override model.fit\n",
        "\n",
        "    x, y = data\n",
        "\n",
        "    e_id_SP = x[0] # get seeker id \n",
        "    e_attention_SP = x[1] # get seeker attention \n",
        "    e_id_RP = x[2] # get response id \n",
        "    e_attention_RP = x[3] # get response attention\n",
        "    e_sp = x[4] # get seeker post\n",
        "    e_rp = x[5] # get response post\n",
        "    e_labels = y[0] # get labels\n",
        "    e_rationales = y[1] # get rationales\n",
        "\n",
        "    # get loss + gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits_empathy, logits_rationale = self(x)\n",
        "        loss, loss_empathy, loss_rationale = overall_loss(e_labels, logits_empathy, e_rationales, logits_rationale, e_attention_RP, self.lambda_EI, self.lambda_RE)\n",
        "    \n",
        "    # compute gradients\n",
        "    vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, vars)\n",
        "\n",
        "    # update weights \n",
        "    self.optimizer.apply_gradients(zip(gradients, vars))\n",
        "\n",
        "    # compute our own metrics + update them\n",
        "    loss_tracker.update_state(loss)\n",
        "    empathy_metric.update_state(loss_empathy)\n",
        "    rationale_metric.update_state(loss_rationale)\n",
        "\n",
        "    empathy_accuracy.update_state(e_labels, tf.argmax(tf.reshape(logits_empathy, [-1, 3]), 1))\n",
        "\n",
        "    e_rationales = tf.reduce_mean(e_rationales, 1)\n",
        "\n",
        "    rationale_accuracy.update_state(tf.reshape(e_rationales, [-1]), tf.argmax(tf.reshape(logits_rationale, [-1, 2]), 1))\n",
        "\n",
        "    f1_score = f1(tf.reshape(e_rationales, [-1]), tf.argmax(tf.reshape(logits_rationale, [-1, 2]), 1))\n",
        "    f1_rationale.update_state(f1_score)\n",
        "\n",
        "    return {\"loss\": loss_tracker.result(), \"empathy loss\": empathy_metric.result(), \"rationale loss\": rationale_metric.result(), \"empathy accuracy\": empathy_accuracy.result(), \"rationale accuracy\": rationale_accuracy.result(), \"rationale f1\": f1_rationale.result()}\n",
        "  \n",
        "  def test_step(self, data): # override model.evaluate\n",
        "    x, y = data\n",
        "\n",
        "    e_id_SP = x[0] # get seeker id \n",
        "    e_attention_SP = x[1] # get seeker attention \n",
        "    e_id_RP = x[2] # get response id \n",
        "    e_attention_RP = x[3] # get response attention\n",
        "    e_sp = x[4] # get seeker post\n",
        "    e_rp = x[5] # get response post\n",
        "    e_labels = y[0] # get labels\n",
        "    e_rationales = y[1] # get rationales\n",
        "\n",
        "    logits_empathy, logits_rationale = self(x)\n",
        "    loss, loss_empathy, loss_rationale = overall_loss(e_labels, logits_empathy, e_rationales, logits_rationale, e_attention_RP, self.lambda_EI, self.lambda_RE)\n",
        "    \n",
        "    # compute our own metrics\n",
        "    loss_tracker.update_state(loss)\n",
        "    empathy_metric.update_state(loss_empathy)\n",
        "    rationale_metric.update_state(loss_rationale)\n",
        "\n",
        "    empathy_accuracy.update_state(e_labels, tf.argmax(tf.reshape(logits_empathy, [-1, 3]), 1))\n",
        "\n",
        "    e_rationales = tf.reduce_mean(e_rationales, 1)\n",
        "    rationale_accuracy.update_state(tf.reshape(e_rationales, [-1]), tf.argmax(tf.reshape(logits_rationale, [-1, 2]), 1))\n",
        "    \n",
        "    f1_score = f1(tf.reshape(e_rationales, [-1]), tf.argmax(tf.reshape(logits_rationale, [-1, 2]), 1))\n",
        "    f1_rationale.update_state(f1_score)\n",
        "\n",
        "    return {\"loss\": loss_tracker.result(), \"empathy loss\": empathy_metric.result(), \"rationale loss\": rationale_metric.result(), \"empathy accuracy\": empathy_accuracy.result(), \"rationale accuracy\": rationale_accuracy.result(), \"rationale f1\": f1_rationale.result()}\n",
        "  \n",
        "  @property \n",
        "  def metrics(self):\n",
        "    return [loss_tracker, empathy_metric, rationale_metric, empathy_accuracy, rationale_accuracy, f1_rationale]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RULsqYSOsGDs"
      },
      "source": [
        "# Empathy Identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXn16w96sQRG"
      },
      "source": [
        "# modeled after RobertaClassificationHead in model.py\n",
        "class EmpathyClassifier(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, hidden_dropout_prob=0.1, hidden_size=768, empathy_num_labels=3):\n",
        "    super(EmpathyClassifier, self).__init__()\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(hidden_size, activation=None)\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.out_proj = tf.keras.layers.Dense(empathy_num_labels, activation=None)\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "\n",
        "  def call(self, features):\n",
        "    x = features[:, :]\n",
        "    x = tf.nn.dropout(x, self.hidden_dropout_prob)\n",
        "    x = self.dense(x)\n",
        "    x = self.relu(x)\n",
        "    x = tf.nn.dropout(x, self.hidden_dropout_prob)\n",
        "    x = self.out_proj(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tuq8V76eNPLi"
      },
      "source": [
        "# Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaW5SvR5NOgF"
      },
      "source": [
        "# custom loss functions\n",
        "\n",
        "# custom empathy loss with cross entropy\n",
        "def empathy_loss(labels, logits):\n",
        "  labels = tf.reshape(labels, [-1])\n",
        "  logits = tf.reshape(logits, [-1, 3])\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "  return loss\n",
        "\n",
        "# custom rationale loss with cross entropy\n",
        "def rationale_loss(labels, logits, attention):\n",
        "  active_loss = tf.reshape(attention, [-1])\n",
        "  logits = tf.reshape(logits, [-1, 2])\n",
        "\n",
        "  if active_loss.shape[0] == 1:\n",
        "    labels = tf.reshape(labels, [-1])\n",
        "  else:\n",
        "    labels = tf.reduce_mean(labels, 1)\n",
        "    labels = tf.reshape(labels, [-1])\n",
        "\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "  return loss\n",
        "\n",
        "# custom overall loss\n",
        "def overall_loss(labels_empathy, logits_empathy, labels_rationale, logits_rationale, attention, lambda_EI, lambda_RE):\n",
        "  loss_empathy = empathy_loss(labels_empathy, logits_empathy)\n",
        "  loss_rationale = rationale_loss(labels_rationale, logits_rationale, attention)\n",
        "\n",
        "  loss = lambda_EI * loss_empathy + lambda_RE * loss_rationale\n",
        "  return (loss, loss_empathy, loss_rationale)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEN6n86d7Pjj"
      },
      "source": [
        "# Initializing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oPXTUxUyIUq"
      },
      "source": [
        "# prepare the data\n",
        "# tokenize inputs and attention mask, extract labels and rationales, create x and y to pass to model\n",
        "def prepare(data):\n",
        "  tokenizer_RP = robertabase_tokenizer.batch_encode_plus(data.response_post, add_special_tokens=True, max_length=64, padding='max_length', truncation=True)\n",
        "  input_ids_RP = tokenizer_RP['input_ids']\n",
        "  attention_masks_RP = tokenizer_RP['attention_mask']\n",
        "\n",
        "  tokenizer_SP = robertabase_tokenizer.batch_encode_plus(data.seeker_post, add_special_tokens=True, max_length=64, padding='max_length', truncation=True)\n",
        "  input_ids_SP = tokenizer_SP['input_ids']\n",
        "  attention_masks_SP = tokenizer_RP['attention_mask']\n",
        "\n",
        "  labels = np.array(data.level.values.astype(int))\n",
        "  rationales = data.rationale_labels.values\n",
        "  rationales = [list(map(int, i.split(','))) for i in rationales] # convert to int\n",
        "\n",
        "  sp = data.seeker_post.values\n",
        "  rp = data.response_post.values\n",
        "\n",
        "  x = [tf.convert_to_tensor(input_ids_SP, dtype=tf.float32), tf.convert_to_tensor(attention_masks_SP, dtype=tf.float32), tf.convert_to_tensor(input_ids_RP, dtype=tf.float32), tf.convert_to_tensor(attention_masks_RP, dtype=tf.float32), sp, rp]\n",
        "  y = [tf.convert_to_tensor(labels, dtype=tf.int64), tf.convert_to_tensor(rationales, dtype=tf.int64)]\n",
        "\n",
        "  return x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB1PbYtl7V_o",
        "outputId": "4fa5478f-6aa6-449b-c317-8b7ec33c41b1"
      },
      "source": [
        "# tokenize input\n",
        "print(ER_train.shape)\n",
        "# ER\n",
        "x_train_ER, y_train_ER = prepare(ER_train)\n",
        "x_test_ER, y_test_ER = prepare(ER_test)\n",
        "\n",
        "# IN \n",
        "x_train_IN, y_train_IN = prepare(IN_train)\n",
        "x_test_IN, y_test_IN = prepare(IN_test)\n",
        "\n",
        "# EX \n",
        "x_train_EX, y_train_EX = prepare(EX_train)\n",
        "x_test_EX, y_test_EX = prepare(EX_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2467, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBc_YLvQFoxM"
      },
      "source": [
        "# initialize the models\n",
        "\n",
        "ER_model = Model()\n",
        "IN_model = Model()\n",
        "EX_model = Model()\n",
        "\n",
        "optimizer_model = tfa.optimizers.AdamW(\n",
        "    weight_decay=1e-2, # not given but default in py torch so probably used\n",
        "    learning_rate=2e-5, # given\n",
        "    epsilon=1e-8 # given\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbyGRCtYivp3"
      },
      "source": [
        "tf.random.set_seed(12) # given seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04WpSlQgTfoA"
      },
      "source": [
        "checkpoint_path_ER = \"/content/drive/My Drive/empathy/training_empathy_ER/cp.ckpt\"\n",
        "checkpoint_path_IN = \"/content/drive/My Drive/empathy/training_empathy_IN/cp.ckpt\"\n",
        "checkpoint_path_EX = \"/content/drive/My Drive/empathy/training_empathy_EX/cp.ckpt\"\n",
        "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback_ER = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_ER,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "cp_callback_IN = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_IN,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "cp_callback_EX = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_EX,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP1w6Pw3UWgt"
      },
      "source": [
        "# ER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY3lwN0awKl7",
        "outputId": "af9e7ee8-1ab7-454d-fab4-5c2adf0ded5a"
      },
      "source": [
        "# model training was successful - it has been saved\n",
        "# the next blocks of code will use the saved model to run\n",
        "# uncomment this code to train again\n",
        "\n",
        "\n",
        "ER_model.compile(optimizer=optimizer_model, run_eagerly=True)\n",
        "\n",
        "ER_model.fit(x=x_train_ER,\n",
        "             y=y_train_ER,\n",
        "             epochs=4,\n",
        "             batch_size=32, \n",
        "             callbacks=[cp_callback_ER])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9478 - empathy loss: 0.8948 - rationale loss: 0.5300 - empathy accuracy: 0.6356 - rationale accuracy: 0.9846 - rationale f1: 0.0000e+00 \n",
            "Epoch 00001: saving model to /content/drive/My Drive/empathy/training_empathy_ER/cp.ckpt\n",
            "78/78 [==============================] - 1922s 25s/step - loss: 0.9478 - empathy loss: 0.8948 - rationale loss: 0.5300 - empathy accuracy: 0.6356 - rationale accuracy: 0.9846 - rationale f1: 0.0000e+00\n",
            "Epoch 2/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9293 - empathy loss: 0.8801 - rationale loss: 0.4917 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00002: saving model to /content/drive/My Drive/empathy/training_empathy_ER/cp.ckpt\n",
            "78/78 [==============================] - 1906s 24s/step - loss: 0.9293 - empathy loss: 0.8801 - rationale loss: 0.4917 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 3/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9830 - empathy loss: 0.9354 - rationale loss: 0.4761 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00003: saving model to /content/drive/My Drive/empathy/training_empathy_ER/cp.ckpt\n",
            "78/78 [==============================] - 1887s 24s/step - loss: 0.9830 - empathy loss: 0.9354 - rationale loss: 0.4761 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 4/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0206 - empathy loss: 0.9736 - rationale loss: 0.4694 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00004: saving model to /content/drive/My Drive/empathy/training_empathy_ER/cp.ckpt\n",
            "78/78 [==============================] - 1892s 24s/step - loss: 1.0206 - empathy loss: 0.9736 - rationale loss: 0.4694 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f268e13e190>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_u-uwqNZuym",
        "outputId": "db918838-b3c9-472a-f98e-6e2faef72f90"
      },
      "source": [
        "ER_model_saved = Model()\n",
        "ER_model_saved.load_weights(checkpoint_path_ER)\n",
        "ER_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "\n",
        "\n",
        "# score = ER_model.evaluate(x=x_test_ER, \n",
        "                          # y=y_test_ER)\n",
        "\n",
        "score = ER_model_saved.evaluate(x=x_test_ER,\n",
        "                          y=y_test_ER)\n",
        "\n",
        "print(f'Emotional reaction test scores\\nOverall loss: {score[0]} \\nEmpathy loss: {score[1]} \\nRationale loss: {score[2]} \\nEmpathy accuracy: {score[3]} \\nRationale accuracy: {score[4]} \\nRationale f1: {score[5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 240s 12s/step - loss: 1.0361 - empathy loss: 0.9895 - rationale loss: 0.4658 - empathy accuracy: 0.6548 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Emotional reaction test scores\n",
            "Overall loss: 1.0361191034317017 \n",
            "Empathy loss: 0.9895352721214294 \n",
            "Rationale loss: 0.4658363461494446 \n",
            "Empathy accuracy: 0.6547812223434448 \n",
            "Rationale accuracy: 1.0 \n",
            "Rationale f1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAbZpkG6YwFr",
        "outputId": "09d66b4b-31ff-4b19-f140-9c981f9bdfc6"
      },
      "source": [
        "# loading the model and training for 2 additional epochs (6 total)\n",
        "ER_model_saved = Model()\n",
        "ER_model_saved.load_weights(checkpoint_path_ER)\n",
        "ER_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "ER_model.fit(x=x_train_ER,\n",
        "             y=y_train_ER,\n",
        "             initial_epoch=4,\n",
        "             epochs=6,\n",
        "             batch_size=32, \n",
        "             callbacks=[cp_callback_ER])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/6\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0406 - empathy loss: 0.9940 - rationale loss: 0.4657 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/empathy/training_empathy_ER/cp.ckpt\n",
            "78/78 [==============================] - 1900s 24s/step - loss: 1.0406 - empathy loss: 0.9940 - rationale loss: 0.4657 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 6/6\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0530 - empathy loss: 1.0067 - rationale loss: 0.4638 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00006: saving model to /content/drive/My Drive/empathy/training_empathy_ER/cp.ckpt\n",
            "78/78 [==============================] - 1900s 24s/step - loss: 1.0530 - empathy loss: 1.0067 - rationale loss: 0.4638 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f26321c3a90>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTMcown7KnLA",
        "outputId": "5785763d-ced5-4758-b3f1-181ef8e1e97f"
      },
      "source": [
        "ER_model_saved = Model()\n",
        "ER_model_saved.load_weights(checkpoint_path_ER)\n",
        "ER_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "\n",
        "\n",
        "# score = ER_model.evaluate(x=x_test_ER, \n",
        "                          # y=y_test_ER)\n",
        "\n",
        "score = ER_model_saved.evaluate(x=x_test_ER,\n",
        "                          y=y_test_ER)\n",
        "\n",
        "print(f'Emotional reaction test scores\\nOverall loss: {score[0]} \\nEmpathy loss: {score[1]} \\nRationale loss: {score[2]} \\nEmpathy accuracy: {score[3]} \\nRationale accuracy: {score[4]} \\nRationale f1: {score[5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "20/20 [==============================] - 241s 12s/step - loss: 1.0589 - empathy loss: 1.0127 - rationale loss: 0.4621 - empathy accuracy: 0.6548 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Emotional reaction test scores\n",
            "Overall loss: 1.0589051246643066 \n",
            "Empathy loss: 1.0126922130584717 \n",
            "Rationale loss: 0.46212995052337646 \n",
            "Empathy accuracy: 0.6547812223434448 \n",
            "Rationale accuracy: 1.0 \n",
            "Rationale f1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITxlrP79Zw48",
        "outputId": "7f94b198-d48b-4265-866d-840d309004da"
      },
      "source": [
        "# loading the model and training for 2 additional epochs (8 total)\n",
        "ER_model_saved = Model()\n",
        "ER_model_saved.load_weights(checkpoint_path_ER)\n",
        "ER_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "ER_model.fit(x=x_train_ER,\n",
        "             y=y_train_ER,\n",
        "             initial_epoch=6,\n",
        "             epochs=8,\n",
        "             batch_size=32, \n",
        "             callbacks=[cp_callback_ER])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/8\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0598 - empathy loss: 1.0136 - rationale loss: 0.4625 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00007: saving model to /content/drive/My Drive/empathy/training_empathy_ER/cp.ckpt\n",
            "78/78 [==============================] - 1875s 24s/step - loss: 1.0598 - empathy loss: 1.0136 - rationale loss: 0.4625 - empathy accuracy: 0.6619 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 8/8\n",
            "26/78 [=========>....................] - ETA: 21:09 - loss: 1.0663 - empathy loss: 1.0201 - rationale loss: 0.4616 - empathy accuracy: 0.6394 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zB71QFHZyxE",
        "outputId": "a604ed2f-b81c-4d1b-9c2f-73ebc8ee437d"
      },
      "source": [
        "ER_model_saved = Model()\n",
        "ER_model_saved.load_weights(checkpoint_path_ER)\n",
        "ER_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "\n",
        "\n",
        "# score = ER_model.evaluate(x=x_test_ER, \n",
        "                          # y=y_test_ER)\n",
        "\n",
        "score = ER_model_saved.evaluate(x=x_test_ER,\n",
        "                          y=y_test_ER)\n",
        "\n",
        "print(f'Emotional reaction test scores\\nOverall loss: {score[0]} \\nEmpathy loss: {score[1]} \\nRationale loss: {score[2]} \\nEmpathy accuracy: {score[3]} \\nRationale accuracy: {score[4]} \\nRationale f1: {score[5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "20/20 [==============================] - 240s 12s/step - loss: 1.0632 - empathy loss: 1.0171 - rationale loss: 0.4603 - empathy accuracy: 0.6548 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Emotional reaction test scores\n",
            "Overall loss: 1.0631636381149292 \n",
            "Empathy loss: 1.0171290636062622 \n",
            "Rationale loss: 0.46034476161003113 \n",
            "Empathy accuracy: 0.6547812223434448 \n",
            "Rationale accuracy: 1.0 \n",
            "Rationale f1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "S_VtMPnkJagc",
        "outputId": "2709d3fa-6926-4a60-d599-c0fe542452d7"
      },
      "source": [
        "predictions_empathy, predictions_rationale = ER_model_saved.predict(x=x_test_ER)\n",
        "\n",
        "# print(predictions_empathy, predictions_rationale)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-0686faccfc31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions_empathy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_rationale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mER_model_saved\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test_ER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(predictions_empathy, predictions_rationale)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1787\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1619\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0;34m\"\"\"Runs an evaluation execution with one step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m   1613\u001b[0m           outputs, self.distribute_strategy, reduction='concat')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1314\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1315\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1316\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2890\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2892\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3694\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3695\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3697\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1605\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `test_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \"\"\"\n\u001b[1;32m   1571\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c3aa22dd4f30>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0mrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-7810c3d6f645>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, seeker_post)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mseeker_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseeker_post\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrobertabase_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeker_post\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrobertabase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         )\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         )\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m             )\n\u001b[1;32m    546\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         layer_output = self.bert_output(\n\u001b[1;32m    493\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x, approximate)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mGaussian\u001b[0m \u001b[0mError\u001b[0m \u001b[0mLinear\u001b[0m \u001b[0mUnits\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGELUs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   \"\"\"\n\u001b[0;32m--> 351\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapproximate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(features, approximate, name)\u001b[0m\n\u001b[1;32m   3712\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3713\u001b[0m       return 0.5 * features * (1.0 + math_ops.erf(\n\u001b[0;32m-> 3714\u001b[0;31m           features / math_ops.cast(1.4142135623730951, features.dtype)))\n\u001b[0m\u001b[1;32m   3715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxmfhxEXrBhN"
      },
      "source": [
        "print(predictions_empathy.shape)\n",
        "print(predictions_rationale.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVBYqPHr75fF"
      },
      "source": [
        "predictions_empathy = predictions_empathy.reshape(617, 3) # reshape to get rid of useless [], value has to be correct based on the test set\n",
        "predictions_rationale = predictions_rationale.reshape(617, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv3JhUP5rwt"
      },
      "source": [
        "print(np.argmax(predictions_empathy, axis=1))\n",
        "# print(predictions_empathy)\n",
        "\n",
        "print(np.argmax(predictions_rationale, axis=1))\n",
        "# print(predictions_rationale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw0jZPXxUdOg"
      },
      "source": [
        "# IN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38s3iWs9Ug7m",
        "outputId": "4b8e63e1-8794-482d-da5a-f34e29954349"
      },
      "source": [
        "# model training was successful - it has been saved\n",
        "# the next blocks of code will use the saved model to run\n",
        "# uncomment this code to train again\n",
        "\n",
        "IN_model.compile(optimizer=optimizer_model, run_eagerly=True)\n",
        "\n",
        "IN_model.fit(x=x_train_IN,\n",
        "             y=y_train_IN,\n",
        "             epochs=4,\n",
        "             batch_size=32, \n",
        "             callbacks=[cp_callback_IN])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9038 - empathy loss: 0.8393 - rationale loss: 0.6451 - empathy accuracy: 0.5347 - rationale accuracy: 0.6688 - rationale f1: 0.0000e+00 \n",
            "Epoch 00001: saving model to /content/drive/My Drive/empathy/training_empathy_IN/cp.ckpt\n",
            "78/78 [==============================] - 1863s 24s/step - loss: 0.9038 - empathy loss: 0.8393 - rationale loss: 0.6451 - empathy accuracy: 0.5347 - rationale accuracy: 0.6688 - rationale f1: 0.0000e+00\n",
            "Epoch 2/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8972 - empathy loss: 0.8572 - rationale loss: 0.3993 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00002: saving model to /content/drive/My Drive/empathy/training_empathy_IN/cp.ckpt\n",
            "78/78 [==============================] - 1875s 24s/step - loss: 0.8972 - empathy loss: 0.8572 - rationale loss: 0.3993 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 3/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9281 - empathy loss: 0.8905 - rationale loss: 0.3767 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00003: saving model to /content/drive/My Drive/empathy/training_empathy_IN/cp.ckpt\n",
            "78/78 [==============================] - 1932s 25s/step - loss: 0.9281 - empathy loss: 0.8905 - rationale loss: 0.3767 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 4/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9555 - empathy loss: 0.9175 - rationale loss: 0.3798 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00004: saving model to /content/drive/My Drive/empathy/training_empathy_IN/cp.ckpt\n",
            "78/78 [==============================] - 1929s 25s/step - loss: 0.9555 - empathy loss: 0.9175 - rationale loss: 0.3798 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f263257a750>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTKqhbneUg7m",
        "outputId": "79d70359-3319-453c-ca0a-f8307aaf65c4"
      },
      "source": [
        "IN_model_saved = Model()\n",
        "IN_model_saved.load_weights(checkpoint_path_IN)\n",
        "\n",
        "\n",
        "IN_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "\n",
        "score = IN_model_saved.evaluate(x=x_test_IN,\n",
        "                          y=y_test_IN)\n",
        "\n",
        "print(f'Interpretation test scores\\nOverall loss: {score[0]} \\nEmpathy loss: {score[1]} \\nRationale loss: {score[2]} \\nEmpathy accuracy: {score[3]} \\nRationale accuracy: {score[4]} \\nRationale f1: {score[5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 244s 12s/step - loss: 0.9733 - empathy loss: 0.9349 - rationale loss: 0.3836 - empathy accuracy: 0.4797 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Interpretation test scores\n",
            "Overall loss: 0.9732635617256165 \n",
            "Empathy loss: 0.934906542301178 \n",
            "Rationale loss: 0.3835689127445221 \n",
            "Empathy accuracy: 0.4797406792640686 \n",
            "Rationale accuracy: 1.0 \n",
            "Rationale f1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OaPtoIbe8DY",
        "outputId": "17e2f9f4-2fde-48b5-fb4c-2d8529085512"
      },
      "source": [
        "# loading the model and training for 2 additional epochs (6 total)\n",
        "IN_model_saved = Model()\n",
        "IN_model_saved.load_weights(checkpoint_path_IN)\n",
        "IN_model.compile(optimizer=optimizer_model, run_eagerly=True)\n",
        "\n",
        "IN_model.fit(x=x_train_IN,\n",
        "             y=y_train_IN,\n",
        "             initial_epoch=4,\n",
        "             epochs=6,\n",
        "             batch_size=32, \n",
        "             callbacks=[cp_callback_IN])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/6\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9702 - empathy loss: 0.9316 - rationale loss: 0.3868 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/empathy/training_empathy_IN/cp.ckpt\n",
            "78/78 [==============================] - 1936s 25s/step - loss: 0.9702 - empathy loss: 0.9316 - rationale loss: 0.3868 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 6/6\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9835 - empathy loss: 0.9441 - rationale loss: 0.3939 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00006: saving model to /content/drive/My Drive/empathy/training_empathy_IN/cp.ckpt\n",
            "78/78 [==============================] - 1925s 25s/step - loss: 0.9835 - empathy loss: 0.9441 - rationale loss: 0.3939 - empathy accuracy: 0.5391 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f22a9e83e90>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DIMapPbPfJJz",
        "outputId": "fc142d46-d0c1-4e5a-83b9-2e90844660cf"
      },
      "source": [
        "IN_model_saved = Model()\n",
        "IN_model_saved.load_weights(checkpoint_path_IN)\n",
        "\n",
        "\n",
        "IN_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "\n",
        "score = IN_model_saved.evaluate(x=x_test_IN,\n",
        "                          y=y_test_IN)\n",
        "\n",
        "print(f'Interpretation test scores\\nOverall loss: {score[0]} \\nEmpathy loss: {score[1]} \\nRationale loss: {score[2]} \\nEmpathy accuracy: {score[3]} \\nRationale accuracy: {score[4]} \\nRationale f1: {score[5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).loss\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.weight_decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "20/20 [==============================] - 239s 12s/step - loss: 0.9958 - empathy loss: 0.9561 - rationale loss: 0.3975 - empathy accuracy: 0.4797 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Interpretation test scores\n",
            "Overall loss: 0.9958151578903198 \n",
            "Empathy loss: 0.9560658931732178 \n",
            "Rationale loss: 0.3974931240081787 \n",
            "Empathy accuracy: 0.4797406792640686 \n",
            "Rationale accuracy: 1.0 \n",
            "Rationale f1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "289lHsltfKF7",
        "outputId": "2e8fa002-c887-4f77-a61e-4113073e89f0"
      },
      "source": [
        "# loading the model and training for 2 additional epochs (8 total)\n",
        "IN_model_saved = Model()\n",
        "IN_model_saved.load_weights(checkpoint_path_IN)\n",
        "IN_model.compile(optimizer=optimizer_model, run_eagerly=True)\n",
        "\n",
        "IN_model.fit(x=x_train_IN,\n",
        "             y=y_train_IN,\n",
        "             initial_epoch=6,\n",
        "             epochs=8,\n",
        "             batch_size=32, \n",
        "             callbacks=[cp_callback_IN])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/8\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9851 - empathy loss: 0.9322 - rationale loss: 0.5292 - empathy accuracy: 0.5091 - rationale accuracy: 0.9854 - rationale f1: 0.0000e+00 \n",
            "Epoch 00007: saving model to /content/drive/My Drive/empathy/training_empathy_IN/cp.ckpt\n",
            "78/78 [==============================] - 1841s 24s/step - loss: 0.9851 - empathy loss: 0.9322 - rationale loss: 0.5292 - empathy accuracy: 0.5091 - rationale accuracy: 0.9854 - rationale f1: 0.0000e+00\n",
            "Epoch 8/8\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9767 - empathy loss: 0.9276 - rationale loss: 0.4913 - empathy accuracy: 0.5213 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00008: saving model to /content/drive/My Drive/empathy/training_empathy_IN/cp.ckpt\n",
            "78/78 [==============================] - 1827s 23s/step - loss: 0.9767 - empathy loss: 0.9276 - rationale loss: 0.4913 - empathy accuracy: 0.5213 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efa308cdf50>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-5YZ89zfN4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072f1cd6-186a-4bc2-d095-c4d818f03d97"
      },
      "source": [
        "IN_model_saved = Model()\n",
        "IN_model_saved.load_weights(checkpoint_path_IN)\n",
        "\n",
        "\n",
        "IN_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "\n",
        "score = IN_model_saved.evaluate(x=x_test_IN,\n",
        "                          y=y_test_IN)\n",
        "\n",
        "print(f'Interpretation test scores\\nOverall loss: {score[0]} \\nEmpathy loss: {score[1]} \\nRationale loss: {score[2]} \\nEmpathy accuracy: {score[3]} \\nRationale accuracy: {score[4]} \\nRationale f1: {score[5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).loss\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.weight_decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "20/20 [==============================] - 230s 11s/step - loss: 0.9997 - empathy loss: 0.9518 - rationale loss: 0.4795 - empathy accuracy: 0.5478 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Interpretation test scores\n",
            "Overall loss: 0.9997437000274658 \n",
            "Empathy loss: 0.9517909288406372 \n",
            "Rationale loss: 0.4795263111591339 \n",
            "Empathy accuracy: 0.5478119850158691 \n",
            "Rationale accuracy: 1.0 \n",
            "Rationale f1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra1Hz4V-Ug7n"
      },
      "source": [
        "predictions_empathy, predictions_rationale = IN_model_saved.predict(x=x_test_IN)\n",
        "\n",
        "# print(predictions_empathy, predictions_rationale)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMHxtHD9Ug7n"
      },
      "source": [
        "# print(predictions_empathy.shape)\n",
        "# print(predictions_rationale.shape)\n",
        "\n",
        "predictions_empathy = predictions_empathy.reshape(617, 3)\n",
        "predictions_rationale = predictions_rationale.reshape(617, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hljPopcLUg7n"
      },
      "source": [
        "print(np.argmax(predictions_empathy, axis=1))\n",
        "# print(predictions_empathy)\n",
        "\n",
        "print(np.argmax(predictions_rationale, axis=1))\n",
        "# print(predictions_rationale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLMiaTd8Uiuo"
      },
      "source": [
        "# EX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUYFc1fzUl0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78048b27-2612-44a1-bde4-7c820c428182"
      },
      "source": [
        "# model training was successful - it has been saved\n",
        "# the next blocks of code will use the saved model to run\n",
        "# uncomment this code to train again\n",
        "\n",
        "EX_model.compile(optimizer=optimizer_model, run_eagerly=True)\n",
        "\n",
        "EX_model.fit(x=x_train_EX,\n",
        "             y=y_train_EX,\n",
        "             epochs=4,\n",
        "             batch_size=32, \n",
        "             callbacks=[cp_callback_EX])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6406 - empathy loss: 0.5885 - rationale loss: 0.5207 - empathy accuracy: 0.8448 - rationale accuracy: 0.9392 - rationale f1: 0.0000e+00 \n",
            "Epoch 00001: saving model to /content/drive/My Drive/empathy/training_empathy_EX/cp.ckpt\n",
            "78/78 [==============================] - 1829s 23s/step - loss: 0.6406 - empathy loss: 0.5885 - rationale loss: 0.5207 - empathy accuracy: 0.8448 - rationale accuracy: 0.9392 - rationale f1: 0.0000e+00\n",
            "Epoch 2/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6590 - empathy loss: 0.6168 - rationale loss: 0.4223 - empathy accuracy: 0.8456 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00002: saving model to /content/drive/My Drive/empathy/training_empathy_EX/cp.ckpt\n",
            "78/78 [==============================] - 1862s 24s/step - loss: 0.6590 - empathy loss: 0.6168 - rationale loss: 0.4223 - empathy accuracy: 0.8456 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 3/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7327 - empathy loss: 0.6912 - rationale loss: 0.4150 - empathy accuracy: 0.8456 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00003: saving model to /content/drive/My Drive/empathy/training_empathy_EX/cp.ckpt\n",
            "78/78 [==============================] - 1870s 24s/step - loss: 0.7327 - empathy loss: 0.6912 - rationale loss: 0.4150 - empathy accuracy: 0.8456 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 4/4\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8003 - empathy loss: 0.7584 - rationale loss: 0.4194 - empathy accuracy: 0.8456 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00004: saving model to /content/drive/My Drive/empathy/training_empathy_EX/cp.ckpt\n",
            "78/78 [==============================] - 1827s 23s/step - loss: 0.8003 - empathy loss: 0.7584 - rationale loss: 0.4194 - empathy accuracy: 0.8456 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ef92bea8fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDw2ibECUl0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45965a09-82fa-4e87-8f95-5fd4d1f7d52c"
      },
      "source": [
        "EX_model_saved = Model()\n",
        "EX_model_saved.load_weights(checkpoint_path_EX)\n",
        "\n",
        "\n",
        "EX_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "\n",
        "score = EX_model_saved.evaluate(x=x_test_EX,\n",
        "                          y=y_test_EX)\n",
        "\n",
        "print(f'Explorations test scores\\nOverall loss: {score[0]} \\nEmpathy loss: {score[1]} \\nRationale loss: {score[2]} \\nEmpathy accuracy: {score[3]} \\nRationale accuracy: {score[4]} \\nRationale f1: {score[5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 230s 11s/step - loss: 0.8302 - empathy loss: 0.7880 - rationale loss: 0.4214 - empathy accuracy: 0.8395 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Explorations test scores\n",
            "Overall loss: 0.8301589488983154 \n",
            "Empathy loss: 0.7880163192749023 \n",
            "Rationale loss: 0.4214257299900055 \n",
            "Empathy accuracy: 0.8395462036132812 \n",
            "Rationale accuracy: 1.0 \n",
            "Rationale f1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFee_cLzVuzi",
        "outputId": "b551bfe4-bfba-446e-979c-a2bb0c3f862d"
      },
      "source": [
        "# load and train for 2 additional epochs (6 total)\n",
        "EX_model_saved = Model()\n",
        "EX_model_saved.load_weights(checkpoint_path_EX)\n",
        "\n",
        "EX_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "EX_model_saved.fit(x=x_train_EX,\n",
        "             y=y_train_EX,\n",
        "             initial_epoch = 4,\n",
        "             epochs=6,\n",
        "             batch_size=32, \n",
        "             callbacks=[cp_callback_EX])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/6\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rationale_classifier.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).empathy_classifier.out_proj.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8473 - empathy loss: 0.8048 - rationale loss: 0.4245 - empathy accuracy: 0.8472 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/empathy/training_empathy_EX/cp.ckpt\n",
            "78/78 [==============================] - 1754s 22s/step - loss: 0.8473 - empathy loss: 0.8048 - rationale loss: 0.4245 - empathy accuracy: 0.8472 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 6/6\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8784 - empathy loss: 0.8355 - rationale loss: 0.4289 - empathy accuracy: 0.8472 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00006: saving model to /content/drive/My Drive/empathy/training_empathy_EX/cp.ckpt\n",
            "78/78 [==============================] - 1741s 22s/step - loss: 0.8784 - empathy loss: 0.8355 - rationale loss: 0.4289 - empathy accuracy: 0.8472 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f03da37c550>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kBINGG7V6sB",
        "outputId": "79f359c5-57bb-461b-dd20-bfeb6b71a8fe"
      },
      "source": [
        "EX_model_saved = Model()\n",
        "EX_model_saved.load_weights(checkpoint_path_EX)\n",
        "\n",
        "\n",
        "EX_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "\n",
        "score = EX_model_saved.evaluate(x=x_test_EX,\n",
        "                          y=y_test_EX)\n",
        "\n",
        "print(f'Explorations test scores\\nOverall loss: {score[0]} \\nEmpathy loss: {score[1]} \\nRationale loss: {score[2]} \\nEmpathy accuracy: {score[3]} \\nRationale accuracy: {score[4]} \\nRationale f1: {score[5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 218s 11s/step - loss: 0.9000 - empathy loss: 0.8569 - rationale loss: 0.4306 - empathy accuracy: 0.8331 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Explorations test scores\n",
            "Overall loss: 0.8999569416046143 \n",
            "Empathy loss: 0.8568935394287109 \n",
            "Rationale loss: 0.4306330978870392 \n",
            "Empathy accuracy: 0.8330631852149963 \n",
            "Rationale accuracy: 1.0 \n",
            "Rationale f1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPc96xB5V8f_",
        "outputId": "cf847d74-3c13-4543-abaf-d29b3dbfe3c6"
      },
      "source": [
        "# load and train for 2 additional epochs (8 total)\n",
        "EX_model_saved = Model()\n",
        "EX_model_saved.load_weights(checkpoint_path_EX)\n",
        "\n",
        "EX_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "EX_model_saved.fit(x=x_train_EX,\n",
        "             y=y_train_EX,\n",
        "             initial_epoch = 6,\n",
        "             epochs=8,\n",
        "             batch_size=32, \n",
        "             callbacks=[cp_callback_EX])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/8\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9070 - empathy loss: 0.8637 - rationale loss: 0.4325 - empathy accuracy: 0.8472 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00007: saving model to /content/drive/My Drive/empathy/training_empathy_EX/cp.ckpt\n",
            "78/78 [==============================] - 1726s 22s/step - loss: 0.9070 - empathy loss: 0.8637 - rationale loss: 0.4325 - empathy accuracy: 0.8472 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Epoch 8/8\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9341 - empathy loss: 0.8906 - rationale loss: 0.4353 - empathy accuracy: 0.8472 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00 \n",
            "Epoch 00008: saving model to /content/drive/My Drive/empathy/training_empathy_EX/cp.ckpt\n",
            "78/78 [==============================] - 1752s 22s/step - loss: 0.9341 - empathy loss: 0.8906 - rationale loss: 0.4353 - empathy accuracy: 0.8472 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f03da108d10>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNgYEwgKV7Ue",
        "outputId": "81ca3c11-f269-44ca-9909-64f4502b3152"
      },
      "source": [
        "EX_model_saved = Model()\n",
        "EX_model_saved.load_weights(checkpoint_path_EX)\n",
        "\n",
        "\n",
        "EX_model_saved.compile(optimizer=optimizer_model, run_eagerly=True) # recompile\n",
        "\n",
        "score = EX_model_saved.evaluate(x=x_test_EX,\n",
        "                          y=y_test_EX)\n",
        "\n",
        "print(f'Explorations test scores\\nOverall loss: {score[0]} \\nEmpathy loss: {score[1]} \\nRationale loss: {score[2]} \\nEmpathy accuracy: {score[3]} \\nRationale accuracy: {score[4]} \\nRationale f1: {score[5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 222s 11s/step - loss: 0.9516 - empathy loss: 0.9080 - rationale loss: 0.4366 - empathy accuracy: 0.8331 - rationale accuracy: 1.0000 - rationale f1: 0.0000e+00\n",
            "Explorations test scores\n",
            "Overall loss: 0.9516453146934509 \n",
            "Empathy loss: 0.907990038394928 \n",
            "Rationale loss: 0.43655163049697876 \n",
            "Empathy accuracy: 0.8330631852149963 \n",
            "Rationale accuracy: 1.0 \n",
            "Rationale f1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsPcHK5EUl0p"
      },
      "source": [
        "predictions_empathy, predictions_rationale = EX_model_saved.predict(x=x_test_EX)\n",
        "\n",
        "# print(predictions_empathy, predictions_rationale)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC5a7C_2Ul0p"
      },
      "source": [
        "# print(predictions_empathy.shape)\n",
        "# print(predictions_rationale.shape)\n",
        "\n",
        "predictions_empathy = predictions_empathy.reshape(617, 3)\n",
        "predictions_rationale = predictions_rationale.reshape(617, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJDiSa2IUl0q"
      },
      "source": [
        "print(np.argmax(predictions_empathy, axis=1))\n",
        "# print(predictions_empathy)\n",
        "\n",
        "print(np.argmax(predictions_rationale, axis=1))\n",
        "# print(predictions_rationale)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}